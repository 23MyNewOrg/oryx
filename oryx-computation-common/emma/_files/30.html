<HTML><HEAD><META CONTENT="text/html; charset=UTF-8" HTTP-EQUIV="Content-Type"/><TITLE>EMMA Coverage Report</TITLE><STYLE TYPE="text/css"> TABLE,TD,TH {border-style:solid; border-color:black;} TD,TH {background:white;margin:0;line-height:100%;padding-left:0.5em;padding-right:0.5em;} TD {border-width:0 1px 0 0;} TH {border-width:1px 1px 1px 0;} TR TD.h {color:red;} TABLE {border-spacing:0; border-collapse:collapse;border-width:0 0 1px 1px;} P,H1,H2,H3,TH {font-family:verdana,arial,sans-serif;font-size:10pt;} TD {font-family:courier,monospace;font-size:10pt;} TABLE.hdft {border-spacing:0;border-collapse:collapse;border-style:none;} TABLE.hdft TH,TABLE.hdft TD {border-style:none;line-height:normal;} TABLE.hdft TH.tl,TABLE.hdft TD.tl {background:#6699CC;color:white;} TABLE.hdft TD.nv {background:#6633DD;color:white;} .nv A:link {color:white;} .nv A:visited {color:white;} .nv A:active {color:yellow;} TABLE.hdft A:link {color:white;} TABLE.hdft A:visited {color:white;} TABLE.hdft A:active {color:yellow;} .in {color:#356085;} TABLE.s TD {padding-left:0.25em;padding-right:0.25em;} TABLE.s TD.l {padding-left:0.25em;padding-right:0.25em;text-align:right;background:#F0F0F0;} TABLE.s TR.z TD {background:#FF9999;} TABLE.s TR.p TD {background:#FFFF88;} TABLE.s TR.c TD {background:#CCFFCC;} A:link {color:#0000EE;text-decoration:none;} A:visited {color:#0000EE;text-decoration:none;} A:hover {color:#0000EE;text-decoration:underline;} TABLE.cn {border-width:0 0 1px 0;} TABLE.s {border-width:1px 0 1px 1px;} TD.h {color:red;border-width:0 1px 0 0;} TD.f {border-width:0 1px 0 1px;} TD.hf {color:red;border-width:0 1px 0 1px;} TH.f {border-width:1px 1px 1px 1px;} TR.cis TD {background:#F0F0F0;} TR.cis TD {border-width:1px 1px 1px 0;} TR.cis TD.h {color:red;border-width:1px 1px 1px 0;} TR.cis TD.f {border-width:1px 1px 1px 1px;} TR.cis TD.hf {color:red;border-width:1px 1px 1px 1px;} TD.b {border-style:none;background:transparent;line-height:50%;}  TD.bt {border-width:1px 0 0 0;background:transparent;line-height:50%;} TR.o TD {background:#F0F0F0;}TABLE.it {border-style:none;}TABLE.it TD,TABLE.it TH {border-style:none;}</STYLE></HEAD><BODY><TABLE WIDTH="100%" CLASS="hdft" CELLSPACING="0"><TR><TH CLASS="tl"><A HREF="http://emma.sourceforge.net/">EMMA</A> Coverage Report (generated Sat Dec 13 23:33:41 GMT 2014)</TH></TR><TR><TD CLASS="nv">[<A HREF="../index.html">all classes</A>][<A HREF="9.html">com.cloudera.oryx.computation.common</A>]</TD></TR></TABLE><H2>COVERAGE SUMMARY FOR SOURCE FILE [<SPAN CLASS="in">JobStep.java</SPAN>]</H2><TABLE WIDTH="100%" CELLSPACING="0"><TR><TH>name</TH><TH>class, %</TH><TH>method, %</TH><TH>block, %</TH><TH>line, %</TH></TR><TR><TD>JobStep.java</TD><TD CLASS="h">0%   (0/2)</TD><TD CLASS="h">0%   (0/29)</TD><TD CLASS="h">0%   (0/866)</TD><TD CLASS="h">0%   (0/179)</TD></TR></TABLE><H3>COVERAGE BREAKDOWN BY CLASS AND METHOD</H3><TABLE WIDTH="100%" CLASS="cn" CELLSPACING="0"><TR><TH CLASS="f">name</TH><TH>class, %</TH><TH>method, %</TH><TH>block, %</TH><TH>line, %</TH></TR><TR><TD CLASS="b"> </TD><TD CLASS="b"> </TD><TD CLASS="b"> </TD><TD CLASS="b"> </TD><TD CLASS="b"> </TD></TR><TR CLASS="cis"><TD CLASS="f">class <A HREF="#0">JobStep</A></TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/28)</TD><TD CLASS="h">0%   (0/826)</TD><TD CLASS="h">0%   (0/179)</TD></TR><TR><TD CLASS="f"><A HREF="#1">&lt;static initializer&gt;</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/4)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR CLASS="o"><TD CLASS="f"><A HREF="#0">JobStep (): void</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/3)</TD><TD CLASS="h">0%   (0/2)</TD></TR><TR><TD CLASS="f"><A HREF="#3">avroInput (String, PType): Source</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/6)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR CLASS="o"><TD CLASS="f"><A HREF="#4">avroOutput (String): Target</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/4)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR><TD CLASS="f"><A HREF="#5">compressedTextOutput (Configuration, String): Target</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/14)</TD><TD CLASS="h">0%   (0/3)</TD></TR><TR CLASS="o"><TD CLASS="f"><A HREF="#6">createBasicPipeline (Class): MRPipeline</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/231)</TD><TD CLASS="h">0%   (0/48)</TD></TR><TR><TD CLASS="f"><A HREF="#7">createPipeline (): MRPipeline</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/2)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR CLASS="o"><TD CLASS="f"><A HREF="#8">defaultCustomJobName (): String</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/23)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR><TD CLASS="f"><A HREF="#9">determineProgresses (): float []</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/62)</TD><TD CLASS="h">0%   (0/11)</TD></TR><TR CLASS="o"><TD CLASS="f"><A HREF="#a">determineStatus (): StepStatus</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/91)</TD><TD CLASS="h">0%   (0/19)</TD></TR><TR><TD CLASS="f"><A HREF="#b">getConfig (): JobStepConfig</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/3)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR CLASS="o"><TD CLASS="f"><A HREF="#c">getCustomJobName (): String</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/3)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR><TD CLASS="f"><A HREF="#d">getJob (): JobContext</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/13)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR CLASS="o"><TD CLASS="f"><A HREF="#e">getNumReducers (): int</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/13)</TD><TD CLASS="h">0%   (0/4)</TD></TR><TR><TD CLASS="f"><A HREF="#f">getStepStates (): Collection</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/44)</TD><TD CLASS="h">0%   (0/11)</TD></TR><TR CLASS="o"><TD CLASS="f"><A HREF="#10">groupWithComparator (Class): GroupingOptions</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/5)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR><TD CLASS="f"><A HREF="#11">groupingOptions (): GroupingOptions</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/4)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR CLASS="o"><TD CLASS="f"><A HREF="#12">groupingOptions (Class, Class): GroupingOptions</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/6)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR><TD CLASS="f"><A HREF="#13">groupingOptions (Class, Class, Class): GroupingOptions</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/22)</TD><TD CLASS="h">0%   (0/8)</TD></TR><TR CLASS="o"><TD CLASS="f"><A HREF="#14">input (String, PType): Source</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/5)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR><TD CLASS="f"><A HREF="#15">isHighMemoryStep (): boolean</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/2)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR CLASS="o"><TD CLASS="f"><A HREF="#16">output (String): Target</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/4)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR><TD CLASS="f"><A HREF="#17">postRun (): void</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR CLASS="o"><TD CLASS="f"><A HREF="#18">run (String []): int</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/145)</TD><TD CLASS="h">0%   (0/31)</TD></TR><TR><TD CLASS="f"><A HREF="#19">run (Tool, String []): void</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/44)</TD><TD CLASS="h">0%   (0/14)</TD></TR><TR CLASS="o"><TD CLASS="f"><A HREF="#1a">textInput (String): Source</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/4)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR><TD CLASS="f"><A HREF="#1b">toString (): String</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/4)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR CLASS="o"><TD CLASS="f"><A HREF="#1c">validOutputPath (String): boolean</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/64)</TD><TD CLASS="h">0%   (0/12)</TD></TR><TR><TD CLASS="bt"> </TD><TD CLASS="bt"> </TD><TD CLASS="bt"> </TD><TD CLASS="bt"> </TD><TD CLASS="bt"> </TD></TR><TR CLASS="cis"><TD CLASS="f">class <A HREF="#1d">JobStep$1</A></TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/40)</TD><TD CLASS="h">0%   (0/1)</TD></TR><TR><TD CLASS="f"><A HREF="#1d">&lt;static initializer&gt;</A></TD><TD> </TD><TD CLASS="h">0%   (0/1)</TD><TD CLASS="h">0%   (0/40)</TD><TD CLASS="h">0%   (0/1)</TD></TR></TABLE><P></P><TABLE WIDTH="100%" CLASS="s" CELLSPACING="0"><TR><TD CLASS="l">1</TD><TD>/*</TD></TR><TR><TD CLASS="l">2</TD><TD> * Copyright (c) 2013, Cloudera, Inc. All Rights Reserved.</TD></TR><TR><TD CLASS="l">3</TD><TD> *</TD></TR><TR><TD CLASS="l">4</TD><TD> * Cloudera, Inc. licenses this file to you under the Apache License,</TD></TR><TR><TD CLASS="l">5</TD><TD> * Version 2.0 (the &#34;License&#34;). You may not use this file except in</TD></TR><TR><TD CLASS="l">6</TD><TD> * compliance with the License. You may obtain a copy of the License at</TD></TR><TR><TD CLASS="l">7</TD><TD> *</TD></TR><TR><TD CLASS="l">8</TD><TD> *     http://www.apache.org/licenses/LICENSE-2.0</TD></TR><TR><TD CLASS="l">9</TD><TD> *</TD></TR><TR><TD CLASS="l">10</TD><TD> * This software is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR</TD></TR><TR><TD CLASS="l">11</TD><TD> * CONDITIONS OF ANY KIND, either express or implied. See the License for</TD></TR><TR><TD CLASS="l">12</TD><TD> * the specific language governing permissions and limitations under the</TD></TR><TR><TD CLASS="l">13</TD><TD> * License.</TD></TR><TR><TD CLASS="l">14</TD><TD> */</TD></TR><TR><TD CLASS="l">15</TD><TD> </TD></TR><TR><TD CLASS="l">16</TD><TD>package com.cloudera.oryx.computation.common;</TD></TR><TR><TD CLASS="l">17</TD><TD> </TD></TR><TR><TD CLASS="l">18</TD><TD>import java.io.IOException;</TD></TR><TR><TD CLASS="l">19</TD><TD>import java.util.Arrays;</TD></TR><TR><TD CLASS="l">20</TD><TD>import java.util.Collection;</TD></TR><TR><TD CLASS="l">21</TD><TD>import java.util.Collections;</TD></TR><TR><TD CLASS="l">22</TD><TD>import java.util.Date;</TD></TR><TR><TD CLASS="l">23</TD><TD> </TD></TR><TR><TD CLASS="l">24</TD><TD>import com.google.common.base.Preconditions;</TD></TR><TR><TD CLASS="l">25</TD><TD>import com.typesafe.config.Config;</TD></TR><TR><TD CLASS="l">26</TD><TD>import org.apache.crunch.GroupingOptions;</TD></TR><TR><TD CLASS="l">27</TD><TD>import org.apache.crunch.PipelineExecution;</TD></TR><TR><TD CLASS="l">28</TD><TD>import org.apache.crunch.Source;</TD></TR><TR><TD CLASS="l">29</TD><TD>import org.apache.crunch.Target;</TD></TR><TR><TD CLASS="l">30</TD><TD>import org.apache.crunch.impl.mr.MRPipeline;</TD></TR><TR><TD CLASS="l">31</TD><TD>import org.apache.crunch.impl.mr.MRPipelineExecution;</TD></TR><TR><TD CLASS="l">32</TD><TD>import org.apache.crunch.io.From;</TD></TR><TR><TD CLASS="l">33</TD><TD>import org.apache.crunch.io.To;</TD></TR><TR><TD CLASS="l">34</TD><TD>import org.apache.crunch.types.PType;</TD></TR><TR><TD CLASS="l">35</TD><TD>import org.apache.crunch.types.avro.AvroType;</TD></TR><TR><TD CLASS="l">36</TD><TD>import org.apache.hadoop.conf.Configuration;</TD></TR><TR><TD CLASS="l">37</TD><TD>import org.apache.hadoop.conf.Configured;</TD></TR><TR><TD CLASS="l">38</TD><TD>import org.apache.hadoop.io.RawComparator;</TD></TR><TR><TD CLASS="l">39</TD><TD>import org.apache.hadoop.io.SequenceFile;</TD></TR><TR><TD CLASS="l">40</TD><TD>import org.apache.hadoop.io.compress.CompressionCodec;</TD></TR><TR><TD CLASS="l">41</TD><TD>import org.apache.hadoop.io.compress.GzipCodec;</TD></TR><TR><TD CLASS="l">42</TD><TD>import org.apache.hadoop.io.compress.SnappyCodec;</TD></TR><TR><TD CLASS="l">43</TD><TD>import org.apache.hadoop.mapreduce.Cluster;</TD></TR><TR><TD CLASS="l">44</TD><TD>import org.apache.hadoop.mapreduce.Job;</TD></TR><TR><TD CLASS="l">45</TD><TD>import org.apache.hadoop.mapreduce.JobContext;</TD></TR><TR><TD CLASS="l">46</TD><TD>import org.apache.hadoop.mapreduce.JobID;</TD></TR><TR><TD CLASS="l">47</TD><TD>import org.apache.hadoop.mapreduce.JobStatus;</TD></TR><TR><TD CLASS="l">48</TD><TD>import org.apache.hadoop.mapreduce.MRJobConfig;</TD></TR><TR><TD CLASS="l">49</TD><TD>import org.apache.hadoop.mapreduce.Partitioner;</TD></TR><TR><TD CLASS="l">50</TD><TD>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</TD></TR><TR><TD CLASS="l">51</TD><TD>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</TD></TR><TR><TD CLASS="l">52</TD><TD>import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;</TD></TR><TR><TD CLASS="l">53</TD><TD>import org.apache.hadoop.mapreduce.lib.partition.HashPartitioner;</TD></TR><TR><TD CLASS="l">54</TD><TD>import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig;</TD></TR><TR><TD CLASS="l">55</TD><TD>import org.apache.hadoop.util.Tool;</TD></TR><TR><TD CLASS="l">56</TD><TD>import org.apache.hadoop.util.ToolRunner;</TD></TR><TR><TD CLASS="l">57</TD><TD>import org.slf4j.Logger;</TD></TR><TR><TD CLASS="l">58</TD><TD>import org.slf4j.LoggerFactory;</TD></TR><TR><TD CLASS="l">59</TD><TD> </TD></TR><TR><TD CLASS="l">60</TD><TD>import com.cloudera.oryx.common.log.MemoryHandler;</TD></TR><TR><TD CLASS="l">61</TD><TD>import com.cloudera.oryx.common.settings.ConfigUtils;</TD></TR><TR><TD CLASS="l">62</TD><TD>import com.cloudera.oryx.common.servcomp.Namespaces;</TD></TR><TR><TD CLASS="l">63</TD><TD>import com.cloudera.oryx.common.servcomp.OryxConfiguration;</TD></TR><TR><TD CLASS="l">64</TD><TD>import com.cloudera.oryx.common.servcomp.Store;</TD></TR><TR><TD CLASS="l">65</TD><TD> </TD></TR><TR><TD CLASS="l">66</TD><TD>/**</TD></TR><TR><TD CLASS="l"><A NAME="0">67</A></TD><TD> * Abstract superclass of all &#34;steps&#34;.</TD></TR><TR><TD CLASS="l">68</TD><TD> *</TD></TR><TR><TD CLASS="l"><A NAME="1">69</A></TD><TD> * @author Sean Owen</TD></TR><TR><TD CLASS="l">70</TD><TD> */</TD></TR><TR CLASS="z"><TD CLASS="l">71</TD><TD>public abstract class JobStep extends Configured implements Tool, HasState {</TD></TR><TR><TD CLASS="l">72</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">73</TD><TD>  private static final Logger log = LoggerFactory.getLogger(JobStep.class);</TD></TR><TR><TD CLASS="l">74</TD><TD> </TD></TR><TR><TD CLASS="l">75</TD><TD>  public static final String CONFIG_SERIALIZATION_KEY = &#34;CONFIG_SERIALIZATION&#34;;</TD></TR><TR><TD CLASS="l">76</TD><TD> </TD></TR><TR><TD CLASS="l">77</TD><TD>  private JobStepConfig config;</TD></TR><TR><TD CLASS="l">78</TD><TD>  private Date startTime;</TD></TR><TR><TD CLASS="l">79</TD><TD>  private Date endTime;</TD></TR><TR><TD CLASS="l">80</TD><TD>  private MRPipelineExecution exec;</TD></TR><TR><TD CLASS="l"><A NAME="b">81</A></TD><TD> </TD></TR><TR><TD CLASS="l">82</TD><TD>  protected abstract JobStepConfig parseConfig(String[] args);</TD></TR><TR><TD CLASS="l">83</TD><TD> </TD></TR><TR><TD CLASS="l">84</TD><TD>  protected JobStepConfig getConfig() {</TD></TR><TR CLASS="z"><TD CLASS="l">85</TD><TD>    return config;</TD></TR><TR><TD CLASS="l"><A NAME="f">86</A></TD><TD>  }</TD></TR><TR><TD CLASS="l">87</TD><TD> </TD></TR><TR><TD CLASS="l">88</TD><TD>  @Override</TD></TR><TR><TD CLASS="l">89</TD><TD>  public final Collection&lt;StepState&gt; getStepStates() throws IOException, InterruptedException {</TD></TR><TR CLASS="z"><TD CLASS="l">90</TD><TD>    String name = getCustomJobName();</TD></TR><TR CLASS="z"><TD CLASS="l">91</TD><TD>    StepStatus status = determineStatus();</TD></TR><TR CLASS="z"><TD CLASS="l">92</TD><TD>    StepState state = new StepState(startTime, endTime, name, status);</TD></TR><TR CLASS="z"><TD CLASS="l">93</TD><TD>    if (status == StepStatus.COMPLETED) {</TD></TR><TR><TD CLASS="l">94</TD><TD>      // This completed, -- perhaps without ever running, which means Hadoop doesn't report state because</TD></TR><TR><TD CLASS="l">95</TD><TD>      // it was skipped. It's complete.</TD></TR><TR CLASS="z"><TD CLASS="l">96</TD><TD>      state.setMapProgress(1.0f);</TD></TR><TR CLASS="z"><TD CLASS="l">97</TD><TD>      state.setReduceProgress(1.0f);</TD></TR><TR><TD CLASS="l">98</TD><TD>    } else {</TD></TR><TR CLASS="z"><TD CLASS="l">99</TD><TD>      float[] progresses = determineProgresses();</TD></TR><TR CLASS="z"><TD CLASS="l">100</TD><TD>      if (progresses != null) {</TD></TR><TR><TD CLASS="l">101</TD><TD>        // 0 is setup progress; we don't care</TD></TR><TR CLASS="z"><TD CLASS="l">102</TD><TD>        state.setMapProgress(progresses[1]);</TD></TR><TR CLASS="z"><TD CLASS="l">103</TD><TD>        state.setReduceProgress(progresses[2]);</TD></TR><TR><TD CLASS="l">104</TD><TD>      }</TD></TR><TR><TD CLASS="l">105</TD><TD>    }</TD></TR><TR CLASS="z"><TD CLASS="l"><A NAME="a">106</A></TD><TD>    return Collections.singletonList(state);</TD></TR><TR><TD CLASS="l">107</TD><TD>  }</TD></TR><TR><TD CLASS="l">108</TD><TD> </TD></TR><TR><TD CLASS="l">109</TD><TD>  private StepStatus determineStatus() throws IOException, InterruptedException {</TD></TR><TR CLASS="z"><TD CLASS="l">110</TD><TD>    JobContext job = getJob();</TD></TR><TR CLASS="z"><TD CLASS="l">111</TD><TD>    if (job == null) {</TD></TR><TR CLASS="z"><TD CLASS="l">112</TD><TD>      return StepStatus.COMPLETED;</TD></TR><TR><TD CLASS="l">113</TD><TD>    }</TD></TR><TR CLASS="z"><TD CLASS="l">114</TD><TD>    Cluster cluster = new Cluster(getConf());</TD></TR><TR><TD CLASS="l">115</TD><TD>    try {</TD></TR><TR CLASS="z"><TD CLASS="l">116</TD><TD>      JobID jobID = job.getJobID();</TD></TR><TR CLASS="z"><TD CLASS="l">117</TD><TD>      if (jobID == null) {</TD></TR><TR CLASS="z"><TD CLASS="l">118</TD><TD>        return StepStatus.PENDING;</TD></TR><TR><TD CLASS="l">119</TD><TD>      }</TD></TR><TR CLASS="z"><TD CLASS="l">120</TD><TD>      Job runningJob = cluster.getJob(jobID);</TD></TR><TR CLASS="z"><TD CLASS="l"><A NAME="1d">121</A></TD><TD>      if (runningJob == null) {</TD></TR><TR CLASS="z"><TD CLASS="l">122</TD><TD>        return StepStatus.PENDING;</TD></TR><TR><TD CLASS="l">123</TD><TD>      }</TD></TR><TR CLASS="z"><TD CLASS="l">124</TD><TD>      JobStatus.State state = runningJob.getJobState();</TD></TR><TR CLASS="z"><TD CLASS="l">125</TD><TD>      switch (state) {</TD></TR><TR><TD CLASS="l">126</TD><TD>        case PREP:</TD></TR><TR CLASS="z"><TD CLASS="l">127</TD><TD>          return StepStatus.PENDING;</TD></TR><TR><TD CLASS="l">128</TD><TD>        case RUNNING:</TD></TR><TR CLASS="z"><TD CLASS="l">129</TD><TD>          return StepStatus.RUNNING;</TD></TR><TR><TD CLASS="l">130</TD><TD>        case SUCCEEDED:</TD></TR><TR CLASS="z"><TD CLASS="l">131</TD><TD>          return StepStatus.COMPLETED;</TD></TR><TR><TD CLASS="l">132</TD><TD>        case FAILED:</TD></TR><TR CLASS="z"><TD CLASS="l">133</TD><TD>          return StepStatus.FAILED;</TD></TR><TR><TD CLASS="l">134</TD><TD>        case KILLED:</TD></TR><TR CLASS="z"><TD CLASS="l">135</TD><TD>          return StepStatus.CANCELLED;</TD></TR><TR><TD CLASS="l">136</TD><TD>      }</TD></TR><TR CLASS="z"><TD CLASS="l">137</TD><TD>      throw new IllegalArgumentException(&#34;Unknown Hadoop job state &#34; + state);</TD></TR><TR><TD CLASS="l">138</TD><TD>    } finally {</TD></TR><TR CLASS="z"><TD CLASS="l">139</TD><TD>      cluster.close();</TD></TR><TR><TD CLASS="l"><A NAME="d">140</A></TD><TD>    }</TD></TR><TR><TD CLASS="l">141</TD><TD>  }</TD></TR><TR><TD CLASS="l">142</TD><TD> </TD></TR><TR><TD CLASS="l">143</TD><TD>  private JobContext getJob() {</TD></TR><TR CLASS="z"><TD CLASS="l">144</TD><TD>    return exec == null ? null : exec.getJobs().get(0).getJob();</TD></TR><TR><TD CLASS="l">145</TD><TD>  }</TD></TR><TR><TD CLASS="l">146</TD><TD> </TD></TR><TR><TD CLASS="l"><A NAME="9">147</A></TD><TD>  /**</TD></TR><TR><TD CLASS="l">148</TD><TD>   * @return three progress values, in [0,1], as a {@code float[]}, representing setup, mapper and reducer progress</TD></TR><TR><TD CLASS="l">149</TD><TD>   */</TD></TR><TR><TD CLASS="l">150</TD><TD>  private float[] determineProgresses() throws IOException, InterruptedException {</TD></TR><TR CLASS="z"><TD CLASS="l">151</TD><TD>    if (exec == null) {</TD></TR><TR CLASS="z"><TD CLASS="l">152</TD><TD>      return null;</TD></TR><TR><TD CLASS="l">153</TD><TD>    }</TD></TR><TR CLASS="z"><TD CLASS="l">154</TD><TD>    Cluster cluster = new Cluster(getConf());</TD></TR><TR><TD CLASS="l">155</TD><TD>    try {</TD></TR><TR CLASS="z"><TD CLASS="l">156</TD><TD>      JobID jobID = getJob().getJobID();</TD></TR><TR CLASS="z"><TD CLASS="l">157</TD><TD>      if (jobID == null) {</TD></TR><TR CLASS="z"><TD CLASS="l">158</TD><TD>        return null;</TD></TR><TR><TD CLASS="l">159</TD><TD>      }</TD></TR><TR CLASS="z"><TD CLASS="l">160</TD><TD>      Job runningJob = cluster.getJob(jobID);</TD></TR><TR CLASS="z"><TD CLASS="l">161</TD><TD>      if (runningJob == null) {</TD></TR><TR CLASS="z"><TD CLASS="l">162</TD><TD>        return null;</TD></TR><TR><TD CLASS="l">163</TD><TD>      }</TD></TR><TR><TD CLASS="l">164</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">165</TD><TD>      return new float[] { runningJob.setupProgress(), runningJob.mapProgress(), runningJob.reduceProgress() };</TD></TR><TR><TD CLASS="l">166</TD><TD>    } finally {</TD></TR><TR CLASS="z"><TD CLASS="l">167</TD><TD>      cluster.close();</TD></TR><TR><TD CLASS="l">168</TD><TD>    }</TD></TR><TR><TD CLASS="l">169</TD><TD>  }</TD></TR><TR><TD CLASS="l"><A NAME="18">170</A></TD><TD> </TD></TR><TR><TD CLASS="l">171</TD><TD>  @Override</TD></TR><TR><TD CLASS="l">172</TD><TD>  public final int run(String[] args) throws InterruptedException, IOException, JobException {</TD></TR><TR><TD CLASS="l">173</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">174</TD><TD>    MemoryHandler.setSensibleLogFormat();</TD></TR><TR><TD CLASS="l">175</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">176</TD><TD>    if (log.isInfoEnabled()) {</TD></TR><TR CLASS="z"><TD CLASS="l">177</TD><TD>      log.info(&#34;args are {}&#34;, Arrays.toString(args));</TD></TR><TR><TD CLASS="l">178</TD><TD>    }</TD></TR><TR><TD CLASS="l">179</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">180</TD><TD>    config = parseConfig(args);</TD></TR><TR><TD CLASS="l">181</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">182</TD><TD>    String name = getCustomJobName();</TD></TR><TR CLASS="z"><TD CLASS="l">183</TD><TD>    log.info(&#34;Running {}&#34;, name);</TD></TR><TR><TD CLASS="l">184</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">185</TD><TD>    long start = System.currentTimeMillis();</TD></TR><TR CLASS="z"><TD CLASS="l">186</TD><TD>    startTime = new Date(start);</TD></TR><TR CLASS="z"><TD CLASS="l">187</TD><TD>    MRPipeline pipeline = createPipeline();</TD></TR><TR><TD CLASS="l">188</TD><TD>    try {</TD></TR><TR CLASS="z"><TD CLASS="l">189</TD><TD>      if (pipeline == null) {</TD></TR><TR CLASS="z"><TD CLASS="l">190</TD><TD>        log.info(&#34;{} does not need to run&#34;, name);</TD></TR><TR><TD CLASS="l">191</TD><TD>      } else {</TD></TR><TR><TD CLASS="l">192</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">193</TD><TD>        exec = pipeline.runAsync();</TD></TR><TR CLASS="z"><TD CLASS="l">194</TD><TD>        log.info(&#34;Waiting for {} to complete&#34;, name);</TD></TR><TR CLASS="z"><TD CLASS="l">195</TD><TD>        exec.waitUntilDone();</TD></TR><TR><TD CLASS="l">196</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">197</TD><TD>        PipelineExecution.Status exitStatus = exec.getStatus();</TD></TR><TR CLASS="z"><TD CLASS="l">198</TD><TD>        if (exitStatus == PipelineExecution.Status.KILLED || exitStatus == PipelineExecution.Status.FAILED) {</TD></TR><TR CLASS="z"><TD CLASS="l">199</TD><TD>          exec.kill();</TD></TR><TR CLASS="z"><TD CLASS="l">200</TD><TD>          throw new JobException(name + &#34; failed in state &#34; + exitStatus);</TD></TR><TR><TD CLASS="l">201</TD><TD>        }</TD></TR><TR><TD CLASS="l">202</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">203</TD><TD>        log.info(&#34;Finished {}&#34;, name);</TD></TR><TR CLASS="z"><TD CLASS="l">204</TD><TD>        postRun();</TD></TR><TR><TD CLASS="l">205</TD><TD> </TD></TR><TR><TD CLASS="l">206</TD><TD>      }</TD></TR><TR CLASS="z"><TD CLASS="l">207</TD><TD>    } catch (InterruptedException ie) {</TD></TR><TR CLASS="z"><TD CLASS="l">208</TD><TD>      log.warn(&#34;Interrupted {}&#34;, name);</TD></TR><TR CLASS="z"><TD CLASS="l">209</TD><TD>      exec.kill();</TD></TR><TR CLASS="z"><TD CLASS="l">210</TD><TD>      throw ie;</TD></TR><TR><TD CLASS="l">211</TD><TD>    } finally {</TD></TR><TR CLASS="z"><TD CLASS="l">212</TD><TD>      long end = System.currentTimeMillis();</TD></TR><TR CLASS="z"><TD CLASS="l">213</TD><TD>      endTime = new Date(end);</TD></TR><TR CLASS="z"><TD CLASS="l">214</TD><TD>      log.info(&#34;Completed {} in {}s&#34;, this, (end - start) / 1000L);</TD></TR><TR CLASS="z"><TD CLASS="l">215</TD><TD>      if (pipeline != null) {</TD></TR><TR CLASS="z"><TD CLASS="l">216</TD><TD>        pipeline.done();</TD></TR><TR><TD CLASS="l">217</TD><TD>      }</TD></TR><TR CLASS="z"><TD CLASS="l">218</TD><TD>    }</TD></TR><TR><TD CLASS="l">219</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">220</TD><TD>    return 0;</TD></TR><TR><TD CLASS="l">221</TD><TD>  }</TD></TR><TR><TD CLASS="l">222</TD><TD> </TD></TR><TR><TD CLASS="l">223</TD><TD>  /**</TD></TR><TR><TD CLASS="l">224</TD><TD>   * Subclasses override this to make the {@link MRPipeline}.</TD></TR><TR><TD CLASS="l">225</TD><TD>   *</TD></TR><TR><TD CLASS="l"><A NAME="7">226</A></TD><TD>   * @return {@link MRPipeline} encapsulating the MapReduce to run for this step or null if there is nothing</TD></TR><TR><TD CLASS="l">227</TD><TD>   *  to run</TD></TR><TR><TD CLASS="l">228</TD><TD>   */</TD></TR><TR><TD CLASS="l">229</TD><TD>  protected MRPipeline createPipeline() throws IOException {</TD></TR><TR CLASS="z"><TD CLASS="l">230</TD><TD>    return null;</TD></TR><TR><TD CLASS="l">231</TD><TD>  }</TD></TR><TR><TD CLASS="l">232</TD><TD> </TD></TR><TR><TD CLASS="l">233</TD><TD>  /**</TD></TR><TR><TD CLASS="l"><A NAME="17">234</A></TD><TD>   * Subclasses override this to perform any steps after the job has finished.</TD></TR><TR><TD CLASS="l">235</TD><TD>   */</TD></TR><TR><TD CLASS="l">236</TD><TD>  protected void postRun() throws IOException {</TD></TR><TR><TD CLASS="l"><A NAME="15">237</A></TD><TD>    // do nothing</TD></TR><TR CLASS="z"><TD CLASS="l">238</TD><TD>  }</TD></TR><TR><TD CLASS="l">239</TD><TD> </TD></TR><TR><TD CLASS="l">240</TD><TD>  protected boolean isHighMemoryStep() {</TD></TR><TR CLASS="z"><TD CLASS="l">241</TD><TD>    return false;</TD></TR><TR><TD CLASS="l">242</TD><TD>  }</TD></TR><TR><TD CLASS="l">243</TD><TD> </TD></TR><TR><TD CLASS="l">244</TD><TD>  /**</TD></TR><TR><TD CLASS="l">245</TD><TD>   * Creates a new {@link MRPipeline} instance that contains common configuration</TD></TR><TR><TD CLASS="l">246</TD><TD>   * settings.</TD></TR><TR><TD CLASS="l"><A NAME="6">247</A></TD><TD>   *</TD></TR><TR><TD CLASS="l">248</TD><TD>   * @return a new {@link MRPipeline} instance, suitably configured</TD></TR><TR><TD CLASS="l">249</TD><TD>   */</TD></TR><TR><TD CLASS="l">250</TD><TD>  protected final MRPipeline createBasicPipeline(Class&lt;?&gt; jarClass) throws IOException {</TD></TR><TR CLASS="z"><TD CLASS="l">251</TD><TD>    Configuration conf = OryxConfiguration.get(getConf());</TD></TR><TR><TD CLASS="l">252</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">253</TD><TD>    conf.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true);</TD></TR><TR CLASS="z"><TD CLASS="l">254</TD><TD>    conf.setClass(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, SnappyCodec.class, CompressionCodec.class);</TD></TR><TR><TD CLASS="l">255</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">256</TD><TD>    conf.setBoolean(&#34;mapred.output.compress&#34;, true);</TD></TR><TR CLASS="z"><TD CLASS="l">257</TD><TD>    conf.set(&#34;mapred.output.compression.type&#34;, &#34;BLOCK&#34;);</TD></TR><TR CLASS="z"><TD CLASS="l">258</TD><TD>    conf.setClass(&#34;mapred.output.compression.codec&#34;, SnappyCodec.class, CompressionCodec.class);</TD></TR><TR><TD CLASS="l">259</TD><TD>    // Set old-style equivalents for Avro/Crunch's benefit</TD></TR><TR CLASS="z"><TD CLASS="l">260</TD><TD>    conf.set(&#34;avro.output.codec&#34;, &#34;snappy&#34;);</TD></TR><TR><TD CLASS="l">261</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">262</TD><TD>    conf.setBoolean(MRJobConfig.MAP_SPECULATIVE, true);</TD></TR><TR CLASS="z"><TD CLASS="l">263</TD><TD>    conf.setBoolean(MRJobConfig.REDUCE_SPECULATIVE, true);</TD></TR><TR CLASS="z"><TD CLASS="l">264</TD><TD>    conf.setBoolean(TTConfig.TT_OUTOFBAND_HEARBEAT, true);</TD></TR><TR CLASS="z"><TD CLASS="l">265</TD><TD>    conf.setInt(MRJobConfig.JVM_NUMTASKS_TORUN, -1);</TD></TR><TR><TD CLASS="l">266</TD><TD> </TD></TR><TR><TD CLASS="l">267</TD><TD>    //conf.setBoolean(&#34;crunch.disable.deep.copy&#34;, true);</TD></TR><TR><TD CLASS="l">268</TD><TD>    // Giving one mapper a lot of data can cause issues in some stages, so default to disable this</TD></TR><TR CLASS="z"><TD CLASS="l">269</TD><TD>    conf.setBoolean(&#34;crunch.disable.combine.file&#34;, true);</TD></TR><TR><TD CLASS="l">270</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">271</TD><TD>    Config appConfig = ConfigUtils.getDefaultConfig();</TD></TR><TR><TD CLASS="l">272</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">273</TD><TD>    int mapMemoryMB = appConfig.getInt(&#34;computation-layer.mapper-memory-mb&#34;);</TD></TR><TR CLASS="z"><TD CLASS="l">274</TD><TD>    log.info(&#34;Mapper memory: {}&#34;, mapMemoryMB);</TD></TR><TR CLASS="z"><TD CLASS="l">275</TD><TD>    int mapHeapMB = (int) (mapMemoryMB / 1.3); // Matches Hadoop's default</TD></TR><TR CLASS="z"><TD CLASS="l">276</TD><TD>    log.info(&#34;Mappers have {}MB heap and can access {}MB RAM&#34;, mapHeapMB, mapMemoryMB);</TD></TR><TR CLASS="z"><TD CLASS="l">277</TD><TD>    if (conf.get(MRJobConfig.MAP_JAVA_OPTS) != null) {</TD></TR><TR CLASS="z"><TD CLASS="l">278</TD><TD>      log.info(&#34;Overriding previous setting of {}, which was '{}'&#34;,</TD></TR><TR><TD CLASS="l">279</TD><TD>               MRJobConfig.MAP_JAVA_OPTS,</TD></TR><TR CLASS="z"><TD CLASS="l">280</TD><TD>               conf.get(MRJobConfig.MAP_JAVA_OPTS));</TD></TR><TR><TD CLASS="l">281</TD><TD>    }</TD></TR><TR CLASS="z"><TD CLASS="l">282</TD><TD>    conf.set(MRJobConfig.MAP_JAVA_OPTS,</TD></TR><TR><TD CLASS="l">283</TD><TD>             &#34;-Xmx&#34; + mapHeapMB + &#34;m -XX:+UseCompressedOops -XX:+UseParallelGC -XX:+UseParallelOldGC&#34;);</TD></TR><TR CLASS="z"><TD CLASS="l">284</TD><TD>    log.info(&#34;Set {} to '{}'&#34;, MRJobConfig.MAP_JAVA_OPTS, conf.get(MRJobConfig.MAP_JAVA_OPTS));</TD></TR><TR><TD CLASS="l">285</TD><TD>    // See comment below on CM</TD></TR><TR CLASS="z"><TD CLASS="l">286</TD><TD>    conf.setInt(&#34;mapreduce.map.java.opts.max.heap&#34;, mapHeapMB);</TD></TR><TR><TD CLASS="l">287</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">288</TD><TD>    int reduceMemoryMB = appConfig.getInt(&#34;computation-layer.reducer-memory-mb&#34;);</TD></TR><TR CLASS="z"><TD CLASS="l">289</TD><TD>    log.info(&#34;Reducer memory: {}&#34;, reduceMemoryMB);</TD></TR><TR CLASS="z"><TD CLASS="l">290</TD><TD>    if (isHighMemoryStep()) {</TD></TR><TR CLASS="z"><TD CLASS="l">291</TD><TD>      reduceMemoryMB *= appConfig.getInt(&#34;computation-layer.worker-high-memory-factor&#34;);</TD></TR><TR CLASS="z"><TD CLASS="l">292</TD><TD>      log.info(&#34;Increasing {} to {} for high-memory step&#34;, MRJobConfig.REDUCE_MEMORY_MB, reduceMemoryMB);</TD></TR><TR><TD CLASS="l">293</TD><TD>    }</TD></TR><TR CLASS="z"><TD CLASS="l">294</TD><TD>    conf.setInt(MRJobConfig.REDUCE_MEMORY_MB, reduceMemoryMB);</TD></TR><TR><TD CLASS="l">295</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">296</TD><TD>    int reduceHeapMB = (int) (reduceMemoryMB / 1.3); // Matches Hadoop's default</TD></TR><TR CLASS="z"><TD CLASS="l">297</TD><TD>    log.info(&#34;Reducers have {}MB heap and can access {}MB RAM&#34;, reduceHeapMB, reduceMemoryMB);</TD></TR><TR CLASS="z"><TD CLASS="l">298</TD><TD>    if (conf.get(MRJobConfig.REDUCE_JAVA_OPTS) != null) {</TD></TR><TR CLASS="z"><TD CLASS="l">299</TD><TD>      log.info(&#34;Overriding previous setting of {}, which was '{}'&#34;,</TD></TR><TR><TD CLASS="l">300</TD><TD>               MRJobConfig.REDUCE_JAVA_OPTS,</TD></TR><TR CLASS="z"><TD CLASS="l">301</TD><TD>               conf.get(MRJobConfig.REDUCE_JAVA_OPTS));</TD></TR><TR><TD CLASS="l">302</TD><TD>    }</TD></TR><TR CLASS="z"><TD CLASS="l">303</TD><TD>    conf.set(MRJobConfig.REDUCE_JAVA_OPTS,</TD></TR><TR><TD CLASS="l">304</TD><TD>             &#34;-Xmx&#34; + reduceHeapMB + &#34;m -XX:+UseCompressedOops -XX:+UseParallelGC -XX:+UseParallelOldGC&#34;);</TD></TR><TR CLASS="z"><TD CLASS="l">305</TD><TD>    log.info(&#34;Set {} to '{}'&#34;, MRJobConfig.REDUCE_JAVA_OPTS, conf.get(MRJobConfig.REDUCE_JAVA_OPTS));</TD></TR><TR><TD CLASS="l">306</TD><TD>    // I see this in CM but not in Hadoop docs; probably won't hurt as it's supposed to result in</TD></TR><TR><TD CLASS="l">307</TD><TD>    // -Xmx appended to opts above, which is at worst redundant</TD></TR><TR CLASS="z"><TD CLASS="l">308</TD><TD>    conf.setInt(&#34;mapreduce.reduce.java.opts.max.heap&#34;, reduceHeapMB);</TD></TR><TR><TD CLASS="l">309</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">310</TD><TD>    conf.setInt(&#34;yarn.scheduler.capacity.minimum-allocation-mb&#34;, 128);</TD></TR><TR CLASS="z"><TD CLASS="l">311</TD><TD>    conf.setInt(&#34;yarn.app.mapreduce.am.resource.mb&#34;, 384);</TD></TR><TR><TD CLASS="l">312</TD><TD> </TD></TR><TR><TD CLASS="l">313</TD><TD>    // Pass total config state</TD></TR><TR CLASS="z"><TD CLASS="l">314</TD><TD>    conf.set(CONFIG_SERIALIZATION_KEY, ConfigUtils.getDefaultConfig().root().render());</TD></TR><TR><TD CLASS="l">315</TD><TD> </TD></TR><TR><TD CLASS="l">316</TD><TD>    // Make sure to set any args to conf above this line!</TD></TR><TR><TD CLASS="l">317</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">318</TD><TD>    setConf(conf);</TD></TR><TR><TD CLASS="l">319</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">320</TD><TD>    Job job = Job.getInstance(conf);</TD></TR><TR><TD CLASS="l">321</TD><TD> </TD></TR><TR><TD CLASS="l">322</TD><TD>    // Basic File IO settings</TD></TR><TR CLASS="z"><TD CLASS="l">323</TD><TD>    FileInputFormat.setMaxInputSplitSize(job, 1L &lt;&lt; 28); // ~268MB</TD></TR><TR CLASS="z"><TD CLASS="l">324</TD><TD>    SequenceFileOutputFormat.setOutputCompressionType(job, SequenceFile.CompressionType.BLOCK);</TD></TR><TR CLASS="z"><TD CLASS="l">325</TD><TD>    FileOutputFormat.setCompressOutput(job, true);</TD></TR><TR CLASS="z"><TD CLASS="l">326</TD><TD>    FileOutputFormat.setOutputCompressorClass(job, SnappyCodec.class);</TD></TR><TR><TD CLASS="l">327</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">328</TD><TD>    log.info(&#34;Created pipeline configuration {}&#34;, job.getConfiguration());</TD></TR><TR><TD CLASS="l">329</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l"><A NAME="1c">330</A></TD><TD>    return new MRPipeline(jarClass, getCustomJobName(), job.getConfiguration());</TD></TR><TR><TD CLASS="l">331</TD><TD>  }</TD></TR><TR><TD CLASS="l">332</TD><TD> </TD></TR><TR><TD CLASS="l">333</TD><TD>  protected final boolean validOutputPath(String outputPathKey) throws IOException {</TD></TR><TR CLASS="z"><TD CLASS="l">334</TD><TD>    Preconditions.checkArgument(outputPathKey != null &amp;&amp; outputPathKey.endsWith(&#34;/&#34;),</TD></TR><TR><TD CLASS="l">335</TD><TD>        &#34;%s should end with /&#34;, outputPathKey);</TD></TR><TR><TD CLASS="l">336</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">337</TD><TD>    String jobName = getCustomJobName();</TD></TR><TR><TD CLASS="l">338</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">339</TD><TD>    Store store = Store.get();</TD></TR><TR CLASS="z"><TD CLASS="l">340</TD><TD>    if (store.exists(outputPathKey, false)) {</TD></TR><TR CLASS="z"><TD CLASS="l">341</TD><TD>      log.info(&#34;Output path {} already exists&#34;, outputPathKey);</TD></TR><TR CLASS="z"><TD CLASS="l">342</TD><TD>      if (store.exists(outputPathKey + &#34;_SUCCESS&#34;, true)) {</TD></TR><TR CLASS="z"><TD CLASS="l">343</TD><TD>        log.info(&#34;{} shows _SUCCESS present; skipping&#34;, jobName);</TD></TR><TR CLASS="z"><TD CLASS="l">344</TD><TD>        return false;</TD></TR><TR><TD CLASS="l">345</TD><TD>      }</TD></TR><TR CLASS="z"><TD CLASS="l">346</TD><TD>      log.info(&#34;{} seems to have stopped during an earlier run; deleting output at {} and recomputing&#34;,</TD></TR><TR><TD CLASS="l">347</TD><TD>          jobName, outputPathKey);</TD></TR><TR CLASS="z"><TD CLASS="l">348</TD><TD>      store.recursiveDelete(outputPathKey);</TD></TR><TR><TD CLASS="l">349</TD><TD>    } else {</TD></TR><TR CLASS="z"><TD CLASS="l">350</TD><TD>      log.info(&#34;Output path is clear for writing: {}&#34;, outputPathKey);</TD></TR><TR><TD CLASS="l">351</TD><TD>    }</TD></TR><TR CLASS="z"><TD CLASS="l"><A NAME="14">352</A></TD><TD>    return true;</TD></TR><TR><TD CLASS="l">353</TD><TD>  }</TD></TR><TR><TD CLASS="l">354</TD><TD> </TD></TR><TR><TD CLASS="l">355</TD><TD>  protected final &lt;T&gt; Source&lt;T&gt; input(String inputPathKey, PType&lt;T&gt; ptype) {</TD></TR><TR CLASS="z"><TD CLASS="l"><A NAME="3">356</A></TD><TD>    return avroInput(inputPathKey, ptype);</TD></TR><TR><TD CLASS="l">357</TD><TD>  }</TD></TR><TR><TD CLASS="l">358</TD><TD> </TD></TR><TR><TD CLASS="l">359</TD><TD>  protected final &lt;T&gt; Source&lt;T&gt; avroInput(String inputPathKey, PType&lt;T&gt; avroType) {</TD></TR><TR CLASS="z"><TD CLASS="l"><A NAME="1a">360</A></TD><TD>    return From.avroFile(Namespaces.toPath(inputPathKey), (AvroType&lt;T&gt;) avroType);</TD></TR><TR><TD CLASS="l">361</TD><TD>  }</TD></TR><TR><TD CLASS="l">362</TD><TD> </TD></TR><TR><TD CLASS="l">363</TD><TD>  protected final Source&lt;String&gt; textInput(String inputPathKey) {</TD></TR><TR CLASS="z"><TD CLASS="l"><A NAME="4">364</A></TD><TD>    return From.textFile(Namespaces.toPath(inputPathKey));</TD></TR><TR><TD CLASS="l">365</TD><TD>  }</TD></TR><TR><TD CLASS="l">366</TD><TD> </TD></TR><TR><TD CLASS="l">367</TD><TD>  protected final Target avroOutput(String outputPathKey) {</TD></TR><TR CLASS="z"><TD CLASS="l"><A NAME="16">368</A></TD><TD>    return To.avroFile(Namespaces.toPath(outputPathKey));</TD></TR><TR><TD CLASS="l">369</TD><TD>  }</TD></TR><TR><TD CLASS="l">370</TD><TD> </TD></TR><TR><TD CLASS="l">371</TD><TD>  protected final Target output(String outputPathKey) {</TD></TR><TR CLASS="z"><TD CLASS="l">372</TD><TD>    return avroOutput(outputPathKey);</TD></TR><TR><TD CLASS="l">373</TD><TD>  }</TD></TR><TR><TD CLASS="l"><A NAME="5">374</A></TD><TD> </TD></TR><TR><TD CLASS="l">375</TD><TD>  protected final Target compressedTextOutput(Configuration conf, String outputPathKey) {</TD></TR><TR><TD CLASS="l">376</TD><TD>    // The way this is used, it doesn't seem like we can just set the object in getConf(). Need</TD></TR><TR><TD CLASS="l">377</TD><TD>    // to set the copy in the MRPipeline directly?</TD></TR><TR CLASS="z"><TD CLASS="l">378</TD><TD>    conf.setClass(FileOutputFormat.COMPRESS_CODEC, GzipCodec.class, CompressionCodec.class);</TD></TR><TR CLASS="z"><TD CLASS="l">379</TD><TD>    conf.setClass(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, SnappyCodec.class, CompressionCodec.class);</TD></TR><TR CLASS="z"><TD CLASS="l"><A NAME="11">380</A></TD><TD>    return To.textFile(Namespaces.toPath(outputPathKey));</TD></TR><TR><TD CLASS="l">381</TD><TD>  }</TD></TR><TR><TD CLASS="l">382</TD><TD> </TD></TR><TR><TD CLASS="l">383</TD><TD>  protected final GroupingOptions groupingOptions() {</TD></TR><TR CLASS="z"><TD CLASS="l">384</TD><TD>    return groupWithComparator(null);</TD></TR><TR><TD CLASS="l"><A NAME="10">385</A></TD><TD>  }</TD></TR><TR><TD CLASS="l">386</TD><TD> </TD></TR><TR><TD CLASS="l">387</TD><TD>  protected final GroupingOptions groupWithComparator(</TD></TR><TR><TD CLASS="l">388</TD><TD>      Class&lt;? extends RawComparator&lt;?&gt;&gt; comparator) {</TD></TR><TR CLASS="z"><TD CLASS="l">389</TD><TD>    return groupingOptions(HashPartitioner.class, comparator);</TD></TR><TR><TD CLASS="l">390</TD><TD>  }</TD></TR><TR><TD CLASS="l"><A NAME="12">391</A></TD><TD> </TD></TR><TR><TD CLASS="l">392</TD><TD>  protected final GroupingOptions groupingOptions(</TD></TR><TR><TD CLASS="l">393</TD><TD>      Class&lt;? extends Partitioner&gt; partitionerClass,</TD></TR><TR><TD CLASS="l">394</TD><TD>      Class&lt;? extends RawComparator&lt;?&gt;&gt; comparator) {</TD></TR><TR CLASS="z"><TD CLASS="l">395</TD><TD>    return groupingOptions(partitionerClass, comparator, comparator);</TD></TR><TR><TD CLASS="l">396</TD><TD>  }</TD></TR><TR><TD CLASS="l">397</TD><TD> </TD></TR><TR><TD CLASS="l"><A NAME="13">398</A></TD><TD>  protected final GroupingOptions groupingOptions(</TD></TR><TR><TD CLASS="l">399</TD><TD>      Class&lt;? extends Partitioner&gt; partitionerClass,</TD></TR><TR><TD CLASS="l">400</TD><TD>      Class&lt;? extends RawComparator&lt;?&gt;&gt; groupingComparator,</TD></TR><TR><TD CLASS="l">401</TD><TD>      Class&lt;? extends RawComparator&lt;?&gt;&gt; sortComparator) {</TD></TR><TR CLASS="z"><TD CLASS="l">402</TD><TD>    GroupingOptions.Builder b = GroupingOptions.builder()</TD></TR><TR CLASS="z"><TD CLASS="l">403</TD><TD>        .partitionerClass(partitionerClass)</TD></TR><TR CLASS="z"><TD CLASS="l">404</TD><TD>        .numReducers(getNumReducers());</TD></TR><TR><TD CLASS="l">405</TD><TD> </TD></TR><TR CLASS="z"><TD CLASS="l">406</TD><TD>    if (groupingComparator != null) {</TD></TR><TR CLASS="z"><TD CLASS="l">407</TD><TD>      b.groupingComparatorClass(groupingComparator);</TD></TR><TR><TD CLASS="l">408</TD><TD>    }</TD></TR><TR CLASS="z"><TD CLASS="l">409</TD><TD>    if (sortComparator != null) {</TD></TR><TR CLASS="z"><TD CLASS="l">410</TD><TD>      b.sortComparatorClass(sortComparator);</TD></TR><TR><TD CLASS="l">411</TD><TD>    }</TD></TR><TR CLASS="z"><TD CLASS="l"><A NAME="e">412</A></TD><TD>    return b.build();</TD></TR><TR><TD CLASS="l">413</TD><TD>  }</TD></TR><TR><TD CLASS="l">414</TD><TD> </TD></TR><TR><TD CLASS="l">415</TD><TD>  protected final int getNumReducers() {</TD></TR><TR CLASS="z"><TD CLASS="l">416</TD><TD>    String parallelismString = ConfigUtils.getDefaultConfig().getString(&#34;computation-layer.parallelism&#34;);</TD></TR><TR CLASS="z"><TD CLASS="l">417</TD><TD>    if (&#34;auto&#34;.equals(parallelismString)) {</TD></TR><TR CLASS="z"><TD CLASS="l">418</TD><TD>      return 11; // Default to a prime. Helps avoid problems with funny distributions of IDs.</TD></TR><TR><TD CLASS="l">419</TD><TD>    } else {</TD></TR><TR CLASS="z"><TD CLASS="l">420</TD><TD>      return Integer.parseInt(parallelismString);</TD></TR><TR><TD CLASS="l"><A NAME="19">421</A></TD><TD>    }</TD></TR><TR><TD CLASS="l">422</TD><TD>  }</TD></TR><TR><TD CLASS="l">423</TD><TD> </TD></TR><TR><TD CLASS="l">424</TD><TD>  public static void run(Tool step, String[] args) throws IOException, InterruptedException, JobException {</TD></TR><TR CLASS="z"><TD CLASS="l">425</TD><TD>    log.info(&#34;Running step {}&#34;, step.getClass().getSimpleName());</TD></TR><TR><TD CLASS="l">426</TD><TD>    try {</TD></TR><TR CLASS="z"><TD CLASS="l">427</TD><TD>      int result = ToolRunner.run(OryxConfiguration.get(), step, args);</TD></TR><TR CLASS="z"><TD CLASS="l">428</TD><TD>      if (result != 0) {</TD></TR><TR CLASS="z"><TD CLASS="l">429</TD><TD>        throw new JobException(step + &#34; hasd bad exit status: &#34; + result);</TD></TR><TR><TD CLASS="l">430</TD><TD>      }</TD></TR><TR CLASS="z"><TD CLASS="l">431</TD><TD>    } catch (IOException ioe) {</TD></TR><TR CLASS="z"><TD CLASS="l">432</TD><TD>      throw ioe;</TD></TR><TR CLASS="z"><TD CLASS="l">433</TD><TD>    } catch (InterruptedException ie) {</TD></TR><TR CLASS="z"><TD CLASS="l">434</TD><TD>      throw ie;</TD></TR><TR CLASS="z"><TD CLASS="l">435</TD><TD>    } catch (JobException je) {</TD></TR><TR CLASS="z"><TD CLASS="l">436</TD><TD>      throw je;</TD></TR><TR CLASS="z"><TD CLASS="l">437</TD><TD>    } catch (Exception e) {</TD></TR><TR CLASS="z"><TD CLASS="l">438</TD><TD>      throw new JobException(e);</TD></TR><TR CLASS="z"><TD CLASS="l"><A NAME="8">439</A></TD><TD>    }</TD></TR><TR CLASS="z"><TD CLASS="l">440</TD><TD>  }</TD></TR><TR><TD CLASS="l">441</TD><TD> </TD></TR><TR><TD CLASS="l">442</TD><TD>  protected final String defaultCustomJobName() {</TD></TR><TR CLASS="z"><TD CLASS="l"><A NAME="c">443</A></TD><TD>    return &#34;Oryx-&#34; + config.getInstanceDir() + '-' + config.getGenerationID() + '-' + getClass().getSimpleName();</TD></TR><TR><TD CLASS="l">444</TD><TD>  }</TD></TR><TR><TD CLASS="l">445</TD><TD> </TD></TR><TR><TD CLASS="l">446</TD><TD>  protected String getCustomJobName() {</TD></TR><TR CLASS="z"><TD CLASS="l">447</TD><TD>    return defaultCustomJobName();</TD></TR><TR><TD CLASS="l"><A NAME="1b">448</A></TD><TD>  }</TD></TR><TR><TD CLASS="l">449</TD><TD> </TD></TR><TR><TD CLASS="l">450</TD><TD>  @Override</TD></TR><TR><TD CLASS="l">451</TD><TD>  public final String toString() {</TD></TR><TR CLASS="z"><TD CLASS="l">452</TD><TD>    return getClass().getSimpleName();</TD></TR><TR><TD CLASS="l">453</TD><TD>  }</TD></TR><TR><TD CLASS="l">454</TD><TD> </TD></TR><TR><TD CLASS="l">455</TD><TD>}</TD></TR></TABLE><P></P><TABLE WIDTH="100%" CLASS="hdft" CELLSPACING="0"><TR><TD CLASS="nv">[<A HREF="../index.html">all classes</A>][<A HREF="9.html">com.cloudera.oryx.computation.common</A>]</TD></TR><TR><TD CLASS="tl"><A HREF="http://sourceforge.net/projects/emma">EMMA 2.1.5320 (stable)</A> (C) Vladimir Roubtsov</TD></TR></TABLE></BODY></HTML>